{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":10441994,"datasetId":6463085,"databundleVersionId":10761706}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/lucashmateo/1-using-llm-gpt2-w-hugging-face?scriptVersionId=217080509\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Using LLM's directly with python (without ChatGPT's standard API)\n\nThis is a small project designed to demonstrate the correct way to import and use LLM models.","metadata":{}},{"cell_type":"markdown","source":"## Importing libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport transformers\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-11T09:44:19.980073Z","iopub.execute_input":"2025-01-11T09:44:19.980517Z","iopub.status.idle":"2025-01-11T09:44:40.89407Z","shell.execute_reply.started":"2025-01-11T09:44:19.980485Z","shell.execute_reply":"2025-01-11T09:44:40.893172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T09:44:40.895391Z","iopub.execute_input":"2025-01-11T09:44:40.896092Z","iopub.status.idle":"2025-01-11T09:44:40.900246Z","shell.execute_reply.started":"2025-01-11T09:44:40.896041Z","shell.execute_reply":"2025-01-11T09:44:40.899286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"TensorFlow Version:\", tf.__version__)\nprint(\"Transformers Version:\", transformers.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T09:44:40.902624Z","iopub.execute_input":"2025-01-11T09:44:40.903015Z","iopub.status.idle":"2025-01-11T09:44:40.926546Z","shell.execute_reply.started":"2025-01-11T09:44:40.902983Z","shell.execute_reply":"2025-01-11T09:44:40.925438Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### About the Hugging Face\n\nHugging Face is a popular platform for machine learning models, especially in NLP. Weâ€™ll use the transformers library to load and test the model. To learn more, visit https://huggingface.co/\n\nFor demonstration purposes, the GPT-2 model will be used:\nhttps://transformer.huggingface.co/doc/gpt2-large","metadata":{}},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T09:44:40.92789Z","iopub.execute_input":"2025-01-11T09:44:40.928174Z","iopub.status.idle":"2025-01-11T09:44:45.892155Z","shell.execute_reply.started":"2025-01-11T09:44:40.928149Z","shell.execute_reply":"2025-01-11T09:44:45.890882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the GPT-2 model from Hugging Face","metadata":{}},{"cell_type":"code","source":"model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id = tokenizer.eos_token_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T09:44:45.893197Z","iopub.execute_input":"2025-01-11T09:44:45.893586Z","iopub.status.idle":"2025-01-11T09:45:02.532003Z","shell.execute_reply.started":"2025-01-11T09:44:45.893553Z","shell.execute_reply":"2025-01-11T09:45:02.530394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T09:45:02.534031Z","iopub.execute_input":"2025-01-11T09:45:02.534693Z","iopub.status.idle":"2025-01-11T09:45:02.547768Z","shell.execute_reply.started":"2025-01-11T09:45:02.534643Z","shell.execute_reply":"2025-01-11T09:45:02.546473Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Parameters:\n\n> Padding (pad_token_id):\n\nPadding is the process of adding extra tokens to sequences to make them the same length. This is essential for batching inputs in NLP models, as they require uniform input sizes. Typically, a special token (e.g., [PAD]) is used for this purpose.\nTokenizer.eos_token_id is the maximum sequence length in the batch serves as a reference for padding","metadata":{}},{"cell_type":"code","source":"Image(\"/kaggle/input/padding-image/padding.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T09:48:19.881238Z","iopub.execute_input":"2025-01-11T09:48:19.881789Z","iopub.status.idle":"2025-01-11T09:48:19.968267Z","shell.execute_reply.started":"2025-01-11T09:48:19.881752Z","shell.execute_reply":"2025-01-11T09:48:19.967088Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing the model - text generation with GPT-2","metadata":{}},{"cell_type":"code","source":"prompt = \"What is natural language processing?\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:19:44.07822Z","iopub.execute_input":"2025-01-11T10:19:44.078624Z","iopub.status.idle":"2025-01-11T10:19:44.082936Z","shell.execute_reply.started":"2025-01-11T10:19:44.078594Z","shell.execute_reply":"2025-01-11T10:19:44.082023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input = tokenizer.encode(prompt, return_tensors = 'pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:19:44.227948Z","iopub.execute_input":"2025-01-11T10:19:44.228341Z","iopub.status.idle":"2025-01-11T10:19:44.2348Z","shell.execute_reply.started":"2025-01-11T10:19:44.228295Z","shell.execute_reply":"2025-01-11T10:19:44.233216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Encoding** is the process of converting text into numerical representations that models can understand. Large Language Models (LLMs) and other machine learning models operate on numbers, so encoding bridges the gap between human-readable text and machine-readable data.","metadata":{}},{"cell_type":"code","source":"input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:19:44.512213Z","iopub.execute_input":"2025-01-11T10:19:44.512661Z","iopub.status.idle":"2025-01-11T10:19:44.520472Z","shell.execute_reply.started":"2025-01-11T10:19:44.512622Z","shell.execute_reply":"2025-01-11T10:19:44.519369Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Encoding Tokens**\n\nAbove is the encoded representation of the text: each number corresponds to a token, showing how the model processes the input in a numerical format.","metadata":{}},{"cell_type":"code","source":"output = model.generate(input,\n                        max_length = 200,\n                        num_beams = 10,\n                        no_repeat_ngram_size = 2,\n                        early_stopping = True\n                        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:19:44.844046Z","iopub.execute_input":"2025-01-11T10:19:44.844442Z","iopub.status.idle":"2025-01-11T10:21:53.235953Z","shell.execute_reply.started":"2025-01-11T10:19:44.844405Z","shell.execute_reply":"2025-01-11T10:21:53.234449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Decoding Tokens**\n\nDecoding is the reverse of encoding: it converts the numerical tokens back into human-readable text, allowing us to interpret the model's output.","metadata":{}},{"cell_type":"code","source":"output_text = tokenizer.decode(output[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:21:53.2375Z","iopub.execute_input":"2025-01-11T10:21:53.238046Z","iopub.status.idle":"2025-01-11T10:21:53.248355Z","shell.execute_reply.started":"2025-01-11T10:21:53.238002Z","shell.execute_reply":"2025-01-11T10:21:53.24674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:21:53.250304Z","iopub.execute_input":"2025-01-11T10:21:53.250758Z","iopub.status.idle":"2025-01-11T10:21:53.274046Z","shell.execute_reply.started":"2025-01-11T10:21:53.250722Z","shell.execute_reply":"2025-01-11T10:21:53.272604Z"}},"outputs":[],"execution_count":null}]}